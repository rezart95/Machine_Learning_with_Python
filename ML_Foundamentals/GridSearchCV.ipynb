{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning Multiple Hyperparameters\n",
    "# We could tune them one by one. \n",
    "# If we are tuning a random forest model, we could find the best max_depth, then the best min_leaf_size, then the best n_estimators.\n",
    "# But, what if a specific combination of values works best? \n",
    "# For instance maybe max_depth=5 works best with n_estimators=100, but max_depth=10 works even better when n_estimators=200?\n",
    "# How can we try all possible combinations of multiple hyperparameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'depth 1, min_samples_leaf 3, min_samples_split 2 accuracy': 68.4211,\n",
       " 'depth 1, min_samples_leaf 3, min_samples_split 10 accuracy': 68.4211,\n",
       " 'depth 1, min_samples_leaf 3, min_samples_split 100 accuracy': 68.4211,\n",
       " 'depth 1, min_samples_leaf 15, min_samples_split 2 accuracy': 68.4211,\n",
       " 'depth 1, min_samples_leaf 15, min_samples_split 10 accuracy': 68.4211,\n",
       " 'depth 1, min_samples_leaf 15, min_samples_split 100 accuracy': 68.4211,\n",
       " 'depth 1, min_samples_leaf 20, min_samples_split 2 accuracy': 68.4211,\n",
       " 'depth 1, min_samples_leaf 20, min_samples_split 10 accuracy': 68.4211,\n",
       " 'depth 1, min_samples_leaf 20, min_samples_split 100 accuracy': 68.4211,\n",
       " 'depth 2, min_samples_leaf 3, min_samples_split 2 accuracy': 97.3684,\n",
       " 'depth 2, min_samples_leaf 3, min_samples_split 10 accuracy': 97.3684,\n",
       " 'depth 2, min_samples_leaf 3, min_samples_split 100 accuracy': 68.4211,\n",
       " 'depth 2, min_samples_leaf 15, min_samples_split 2 accuracy': 97.3684,\n",
       " 'depth 2, min_samples_leaf 15, min_samples_split 10 accuracy': 97.3684,\n",
       " 'depth 2, min_samples_leaf 15, min_samples_split 100 accuracy': 68.4211,\n",
       " 'depth 2, min_samples_leaf 20, min_samples_split 2 accuracy': 97.3684,\n",
       " 'depth 2, min_samples_leaf 20, min_samples_split 10 accuracy': 97.3684,\n",
       " 'depth 2, min_samples_leaf 20, min_samples_split 100 accuracy': 68.4211,\n",
       " 'depth 3, min_samples_leaf 3, min_samples_split 2 accuracy': 100.0,\n",
       " 'depth 3, min_samples_leaf 3, min_samples_split 10 accuracy': 100.0,\n",
       " 'depth 3, min_samples_leaf 3, min_samples_split 100 accuracy': 68.4211,\n",
       " 'depth 3, min_samples_leaf 15, min_samples_split 2 accuracy': 97.3684,\n",
       " 'depth 3, min_samples_leaf 15, min_samples_split 10 accuracy': 97.3684,\n",
       " 'depth 3, min_samples_leaf 15, min_samples_split 100 accuracy': 68.4211,\n",
       " 'depth 3, min_samples_leaf 20, min_samples_split 2 accuracy': 97.3684,\n",
       " 'depth 3, min_samples_leaf 20, min_samples_split 10 accuracy': 97.3684,\n",
       " 'depth 3, min_samples_leaf 20, min_samples_split 100 accuracy': 68.4211}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and visualize our parameter grid\n",
    "\n",
    "param_grid = {'max_depth': [1,2,3],\n",
    "              'min_sample_leaf': [3, 15, 20],\n",
    "              'min_samples_split': [2, 10, 100]}\n",
    "\n",
    "pd.DataFrame(param_grid).T\n",
    "\n",
    "# One way we can do this is with nested for loops.\n",
    "\n",
    "scores = {}\n",
    "\n",
    "for d in param_grid['max_depth']:\n",
    "    for l in param_grid['min_sample_leaf']:\n",
    "        for s in param_grid['min_samples_split']:\n",
    "            \n",
    "            #fit a model for each combination of hyperparameter values\n",
    "            model = DecisionTreeClassifier(max_depth=d,\n",
    "                                           min_samples_leaf=l,\n",
    "                                           min_samples_split=s)\n",
    "            \n",
    "            model.fit(X_train, y_train)\n",
    "            score = model.score(X_test, y_test)\n",
    "\n",
    "            # add the model accuracy to a dictionary with the parameter settings as the\n",
    "            # keys and the accuracies as the values.\n",
    "\n",
    "            scores[f'depth {d}, min_samples_leaf {l}, min_samples_split {s} accuracy'] = score.round(6) * 100\n",
    "\n",
    "#display dictionary of scores\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV\n",
    "\n",
    "# GridSearchCV is a model wrapper that will fit many versions of a model with different combinations of hyperparameter settings. \n",
    "# It will even perform cross-validation to more thoroughly check the performance of each model variant.\n",
    "\n",
    "# Just like the code above, GridSearchCV takes a model type (or pipeline!) and a dictionary of hyperparameters and ranges of values for each\n",
    "# hyperparameter. \n",
    "# By default, it will perform a 5-fold cross-validation on a model using each possible combination of hyperparameters values specified in\n",
    "# the parameter grid dictionary.\n",
    "# Then GridSearchCV can automatically select the best model, based on a given scoring function, to return.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first step in using GridSearchCV in Python is to instantiate a model or pipeline and create a parameter grid. \n",
    "# The parameter grid will be a dictionary\n",
    "# with the name of the hyperparameter as the key and a list or range of values for the GridSearchCV to try as the values.\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "param_grid = {'max_depth': [1,2,3],\n",
    "              'min_samples_leaf': [3,15,20],\n",
    "              'min_samples_split': [2, 10, 100]}\n",
    "\n",
    "\n",
    "\n",
    "# If no scoring function is specified, the GridSearchCV object will use the model default .score() function. \n",
    "# For classification models the default is accuracy, and for regression models, the default is R2 score.\n",
    "dt_grid_search = GridSearchCV(model, param_grid)\n",
    "\n",
    "\n",
    "# Fit the GridSearchCV on the training data.\n",
    "# When the GridSearchCV is fit on the training data, it fits many times. It fits and evaluates a new model for every possible combination of hyperparameters\n",
    "# a number of times equal to the number of folds in the cross-validation. By default, this is 5 folds, but it can be adjusted using the ‘cv=’ argument. It\n",
    "# records the scores for each combination of hyperparameters for each fold, as well as the average score for each fold as a dictionary attribute. You can\n",
    "# extract and inspect it later, or you can just retrieve the best model.\n",
    "\n",
    "# While the below is only one line of code, many models are being fit and it can sometimes take a long time to complete.\n",
    "dt_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Examine the best parameters found by the search.\n",
    "dt_grid_search.best_params_\n",
    "\n",
    "\n",
    "# Even though we can extract the best model directly, it’s useful to examine the best parameters that the GridSearchCV object found. For instance, the\n",
    "# search found that the maximum value for depth and minimum values for the samples per split and leaf yielded the best performance. We may want to run\n",
    "# the search again and explore a higher range of values for the max_depth, a lower range for min_samples_leaf, and since 2 is the lowest allowed value for\n",
    "# min_samples_split, we can explore other values nearer to 2 to see if values in these ranges perform even better. For example, we might explore the grid:\n",
    "\n",
    "param_grid2 = {'max_depth': [3, 5, 10],\n",
    "               'min_samples_leaf': [1, 2, 3],\n",
    "               'min_samples_split': [2, 4, 7]}\n",
    "\n",
    "\n",
    "dt_grid_search2 = GridSearchCV(model, param_grid2)\n",
    "dt_grid_search2.fit(X_train, y_train)\n",
    "dt_grid_search2.best_params_\n",
    "\n",
    "# We are zeroing in on the best hyperparameters by adjusting the ranges according to what the previous best hyperparameters were.\n",
    "\n",
    "# If one of the best hyperparameter values were the highest value in a range, we might explore a high range of values for that hyperparameter. If the best\n",
    "# value for a hyperparameter were in the middle, we might tighten my search and explore values close to the best one, above and below.\n",
    "# Notice that I included the best parameters from the first search in the second search in case those were indeed the best values.\n",
    "\n",
    "# Retrieve the best model, refit, and evaluate.\n",
    "\n",
    "#retrieve the best version of the model\n",
    "best_model = dt_grid_search2.best_estimator_\n",
    "\n",
    "#refit the model on the whole training set\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "#score the model on the test set\n",
    "best_model.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV and Pipelines\n",
    "\n",
    "# GridSearchCV uses cross-validation to choose the best values for hyperparameters, so we need to be aware of data leakage when doing any kind of data\n",
    "# preprocessing. GridSearchCV makes it easy to use a pipeline with preprocessing. To do this you would replace the model in the GridSearchCV\n",
    "# constructor with the pipeline.\n",
    "# The difference is in the keys you will need to use for your parameter grid. Since a pipeline contains multiple steps, both the step and the hyperparameter\n",
    "# need to be specified in the parameter grid. The format is: <step>__<hyperparameter>. Notice the double underscore, also called a ‘dunder’ that\n",
    "# separates the step name and the hyperparameter.\n",
    "# For example: if we were going to use a KNei ghborsClassifier and needed to scale our data before fitting, we could use a pipeline to prevent data leakage\n",
    "# during cross-validation. The pipeline.get_params() function will list all available parameters of all steps that can be tuned.\n",
    "\n",
    "\n",
    "knn_pipe = make_pipeline(StandardScaler(), KNeighborsClassifier())\n",
    "knn_pipe.get_params()\n",
    "\n",
    "\n",
    "# We can reference these to use for the key values in our parameter grid dictionary. \n",
    "# Also notice that we are using range() objects to create our set of values for n_neighbors and p.\n",
    "param_grid = {'kneighborsclassifier__n_neighbors': range(1,10),\n",
    "              'kneighborsclassifier__p': range(1,5),\n",
    "              'kneighborsclassifier__weights': ['balanced','uniform']}\n",
    "\n",
    "\n",
    "# From here the process is the same as previously, but we substitute the pipe as the estimator in the first argument of the GridSearchCV constructor.\n",
    "knn_pipe_gs = GridSearchCV(knn_pipe, pipe_param_grid)\n",
    "\n",
    "knn_pipe_gs.fit(X_train, y_train)\n",
    "print('Best KNN Parameters:')\n",
    "\n",
    "print(knn_pipe_gs.best_params_)\n",
    "best_pipe = knn_pipe_gs.best_estimator_\n",
    "print(f'Accuracy of best KNN model is: {best_pipe.score(X_test, y_test)}')\n",
    "\n",
    "\n",
    "# With the above code, we were able to use a pipeline to scale data and tune the hyperparameters of a KNN model using GridSearchCV while avoiding\n",
    "# data leakage during the cross-validation. This can, of course, be done using more complex preprocessing transformers like ColumnTransformer which\n",
    "# includes one-hot encoding, imputation, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The key here is:\n",
    "# The pipeline goes INSIDE the GridSearchCV, in place of the model.\n",
    "# Do NOT put the GridSearchCV inside of the pipeline\n",
    "\n",
    "\n",
    "# Notes on Hyperparameter Searches\n",
    "# Searching for just the right hyperparameters can take time, sometimes a lot of time! While some hyperparameters should always be tuned for each model\n",
    "# you try, such as C for logistic regression, n_neighbors for KNN models, and max_depth for decision tree models, other hyperparameters will make only\n",
    "# small differences. Your data and your base model choice will usually be more impactful than your hyperparameter tuning.\n",
    "\n",
    "#  1. First, work on data.\n",
    "# The choices you make in preparing your dataset, such as how you choose to clean the data, which columns you include, and any feature engineering you\n",
    "# perform will usually have the greatest impact and will generally be where most machine learning engineers spend most of their time.\n",
    "\n",
    "# 2. Second, pick the right model type\n",
    "# Your choice of model type, linear models, clustering models, tree models, ensemble models, etc., will generally have the next biggest impact and it’s a\n",
    "# good idea to try default versions of many model types before starting a hyperparameter search. Again, the exception would be to do some tuning on the\n",
    "# few hyperparameters mentioned above when trying those models. Generally, the models that perform the best with their default settings will also perform\n",
    "# the best after tuning.\n",
    "\n",
    "# 3. Finally, tune hyperparameters.\n",
    "# Hyperparameter tuning, while often very time-intensive, will also often only improve performance a small amount. This should be the last step in your\n",
    "# modeling process and should be reserved for your best 1-3 default models with your best versions of your data.\n",
    "\n",
    "\n",
    "# Additional Options for Tuning\n",
    "# Another tool that is popular among many data scientists is sklearn.model_selection.RandomizedSearchCV. Where GridSearchCV explores all possible\n",
    "# combinations of hyperparameter values in your parameter grid, RandomizedSearchCV explores a random sampling of combinations. Many data\n",
    "# scientists feel that the EXACT right combination is less important than an approximately best set of hyperparameters. You might choose to experiment\n",
    "# with RandomizedSearchCV, especially if you expect an exhaustive search over hyperparameter values will take too long.\n",
    "\n",
    "\n",
    "\n",
    "# Summary\n",
    "# Tuning hyperparameters of a model can improve performance. sklearn.model_selection.GridSearchCV can speed up and simplify the search for optimal\n",
    "# hyperparameter settings. While tuning hyperparameters can improve your model performance, other work can be a better use of limited time, such as\n",
    "# exploring data cleaning and preparation options and testing more model types. Hyperparameter tuning should be the last step in your process after your\n",
    "# data preparation strategy and your base model are chosen."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
