{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipelines and Column Transformers\n",
    "# Pipelines, column selectors, column transformers and other preprocessing transformers can be combined together to perform complex transformations on different subsets of data. For instance you can\n",
    "# impute and scale numeric data and impute and one-hot encode object data.\n",
    "# Pipelines can go inside of ColumnTransformer to make sequential transformation after splitting columns. AND ColumnTransformer objects can be put inside pipelines. You could achieve the\n",
    "# transformations described above with either a series of ColumnTransformer in a Pipeline OR two pipelines inside of a ColumnTransformer. You could even put a ColumnTransformer in a pipeline inside a\n",
    "# ColumnTransformer inside a pipeline!\n",
    "# As you can see, this can get a bit complicated so It can be useful to diagram the transformations you want on your data. Do you want integer numeric data median imputed, float data mean imputed, both\n",
    "# types scaled, object data imputed with the most frequent values and then one-hot encoded?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import make_column_transformer, make_column_selector\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import set_config\n",
    "set_config(display='diagram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 32 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   State               995 non-null    object \n",
      " 1   Lat                 1000 non-null   float64\n",
      " 2   Lng                 1000 non-null   float64\n",
      " 3   Area                995 non-null    object \n",
      " 4   Children            993 non-null    float64\n",
      " 5   Age                 1000 non-null   int64  \n",
      " 6   Income              1000 non-null   float64\n",
      " 7   Marital             995 non-null    object \n",
      " 8   Gender              995 non-null    object \n",
      " 9   ReAdmis             1000 non-null   int64  \n",
      " 10  VitD_levels         1000 non-null   float64\n",
      " 11  Doc_visits          1000 non-null   int64  \n",
      " 12  Full_meals_eaten    1000 non-null   int64  \n",
      " 13  vitD_supp           1000 non-null   int64  \n",
      " 14  Soft_drink          1000 non-null   int64  \n",
      " 15  Initial_admin       995 non-null    object \n",
      " 16  HighBlood           1000 non-null   int64  \n",
      " 17  Stroke              1000 non-null   int64  \n",
      " 18  Complication_risk   995 non-null    object \n",
      " 19  Overweight          1000 non-null   int64  \n",
      " 20  Arthritis           994 non-null    float64\n",
      " 21  Diabetes            994 non-null    float64\n",
      " 22  Hyperlipidemia      998 non-null    float64\n",
      " 23  BackPain            992 non-null    float64\n",
      " 24  Anxiety             998 non-null    float64\n",
      " 25  Allergic_rhinitis   994 non-null    float64\n",
      " 26  Reflux_esophagitis  1000 non-null   int64  \n",
      " 27  Asthma              1000 non-null   int64  \n",
      " 28  Services            995 non-null    object \n",
      " 29  Initial_days        1000 non-null   float64\n",
      " 30  TotalCharge         1000 non-null   float64\n",
      " 31  Additional_charges  1000 non-null   float64\n",
      "dtypes: float64(14), int64(11), object(7)\n",
      "memory usage: 250.1+ KB\n"
     ]
    }
   ],
   "source": [
    "# In this example we will will take data with ordinal categorical, nominal, and numeric data. There are missing data and the numeric data needs to be scaled.\n",
    "# We will use two pipelines, one for numeric and one for categorical and put them in a column transformer. We will also ordinal encode a column before we split the data. None of the missing data is of\n",
    "# integer type.\n",
    "\n",
    "# Load the data\n",
    "path = 'https://docs.google.com/spreadsheets/d/e/2PACX-1vSzb_CfjmApDMSXRn-Ga8X5rgoRVm7U_UNYotqQ0iW2JVx1qoKFr41XOA-FNKPqds83B0oUM6zKtLqK/pub?output=csv'\n",
    "df = pd.read_csv(path)\n",
    "df.head()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 missing values in training data\n",
      "0 missing values in testing data\n",
      "\n",
      "\n",
      "All data in X_train_processed are float64\n",
      "All data in X_test_processed are float64\n",
      "\n",
      "\n",
      "shape of data is (750, 97)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\miniconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.50820472,  0.28193545, -0.06527826, ...,  0.        ,\n",
       "         1.        ,  0.        ],\n",
       "       [-0.72064168,  0.25283631,  1.23912135, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.49340318,  0.48282262, -0.50007813, ...,  0.        ,\n",
       "         1.        ,  0.        ],\n",
       "       ...,\n",
       "       [ 0.27295848,  0.63816773, -0.93487801, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.89653885, -1.73729615, -0.93487801, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.30727477,  1.1082109 , -0.93487801, ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ordinal Encoding\n",
    "# We can ordinal encode data without too much risk of data leakage. There are generally a small number of ordinal variables and they are likely to be in both training and testing data. If that is not the case,\n",
    "# the sklearn transformer called OrdinalEncoder can be added to a preprocessing pipeline.\n",
    "\n",
    "df['Complication_risk'].value_counts()\n",
    "\n",
    "\n",
    "# Ordinal Encoding 'Complication_risk'\n",
    "replacement_dictionary = {'High':2, 'Medium':1, 'Med':1, 'Low':0}\n",
    "df['Complication_risk'].replace(replacement_dictionary, inplace=True)\n",
    "df['Complication_risk']\n",
    "\n",
    "\n",
    "# Validation Split\n",
    "X = df.drop('Additional_charges', axis=1)\n",
    "y = df['Additional_charges']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# Instantiate Column Selectors\n",
    "# We will create our column selectors to use with our column transformer later. We could use lists of columns instead, but a column selector makes it more algorithmic. In this case the code will still work,\n",
    "# even if the columns in the dataframe change after the pipeline has been put into production.\n",
    "\n",
    "# Selectors\n",
    "cat_selector = make_column_selector(dtype_include='object')\n",
    "num_selector = make_column_selector(dtype_include='number')\n",
    "\n",
    "# Instantiate Transformers.\n",
    "# We will be using 3 different transformers, SimpleImputer, StandardScaler, and OneHotEncoder. \n",
    "# There will be 2 different SimpleImputers with different imputation strategies: 'most_frequent' and 'mean'\n",
    "\n",
    "# Imputers\n",
    "freq_imputer = SimpleImputer(strategy='most_frequent')\n",
    "mean_imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# One-hot encoder\n",
    "ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "\n",
    "# Instantiate Pipelines\n",
    "# We will be using TWO different pipelines here. One for numeric data and one for nominal categorical data.\n",
    "\n",
    "# Numeric pipeline\n",
    "numeric_pipe = make_pipeline(mean_imputer, scaler)\n",
    "numeric_pipe\n",
    "\n",
    "# Categorical pipeline\n",
    "categorical_pipe = make_pipeline(freq_imputer, ohe)\n",
    "categorical_pipe\n",
    "\n",
    "\n",
    "# Instantiate ColumnTransformer\n",
    "# make_column_transformer uses tuples to match transformers with the datatypes they should act on. \n",
    "# We can use pipelines as those transformers, which we will do below.\n",
    "\n",
    "# Tuples for Column Transformer\n",
    "number_tuple = (numeric_pipe, num_selector)\n",
    "category_tuple = (categorical_pipe, cat_selector)\n",
    "\n",
    "# ColumnTransformer\n",
    "preprocessor = make_column_transformer(number_tuple, category_tuple)\n",
    "preprocessor\n",
    "\n",
    "# Transformer Data\n",
    "# We fit the ColumnTransformer, which we called 'preprocessor' on the training data. (Never on testing data!)\n",
    "                                                                                    \n",
    "# fit on train\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "# The fit method worked to fit all 4 transformers inside the ColumnTransformer.\n",
    "# We will use that fitted ColumnTransformer to transform both our training and testing datasets.\n",
    "\n",
    "# transform train and test\n",
    "X_train_processed = preprocessor.transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "\n",
    "# Inspect the Result\n",
    "# All Scikit-Learn transformers return Numpy arrays, NOT Pandas dataframes. Because of this we need to use Numpy functions, such as np.isnan(), to inspect our data. In some cases we can easily\n",
    "# transform our data back into a Pandas dataframe, but it's not always easy to get the column names back. The OneHotEncoder created extra columns and it is complicated to retrieve the correct column\n",
    "# names for all columns.\n",
    "# We will ensure that the missing data was replaced, the categorical data was one-hot encoded, and the numeric data was scaled.\n",
    "\n",
    "\n",
    "# Check for missing values and that data is scaled and one-hot encoded\n",
    "print(np.isnan(X_train_processed).sum().sum(), 'missing values in training data')\n",
    "print(np.isnan(X_test_processed).sum().sum(), 'missing values in testing data')\n",
    "print('\\n')\n",
    "print('All data in X_train_processed are', X_train_processed.dtype)\n",
    "print('All data in X_test_processed are', X_test_processed.dtype)\n",
    "print('\\n')\n",
    "print('shape of data is', X_train_processed.shape)\n",
    "print('\\n')\n",
    "X_train_processed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
