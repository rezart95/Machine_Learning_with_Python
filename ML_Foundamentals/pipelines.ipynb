{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is a Pipeline in Machine Learning?\n",
    "\n",
    "A pipeline contains multiple transformers (or even models!) and performs operations on data IN SEQUENCE.  \n",
    "Compare this to ColumnTransformers that perform operations on data IN PARALLEL.  \n",
    "When a pipeline is fit on data, all of the transformers inside it are fit.  \n",
    "When data is transformed using a pipeline, the data is transformed by the first transformer first, the second transformer second, etc.  \n",
    "A pipeline can contain any number of transformers as long as they have .fit() and .transform() methods. These are called 'steps'.  \n",
    "If needed, one single estimator, or model, can be placed at the end of a pipeline.  \n",
    "The important thing to remember is that pipelines are ordered, so the order you use to build them matters.  \n",
    "Pipelines can even contain ColumnTransformer AND ColumnTransformers can contain pipelines.\n",
    "\n",
    "# Why Should I Use Pipelines for Machine Learning?\n",
    "\n",
    "Reasons to use pipelines:\n",
    "\n",
    "1. Pipelines use less code than doing each transformer individually. Since each transformer is fit in a single .fit() call, and the data is transformed by all of the transformers in the pipeline in a single.transform() call, pipelines use significantly less code.\n",
    "2. Pipelines make your preprocessing workflow easier to understand. By reducing the code and displaying a diagram of the pipeline you can show your readers clearly how your data is being transformed before modeling.\n",
    "3. Pipelines are easy to use in production models. When you are ready to deploy your model to use in new data, a preprocessing pipeline can ensure that new data can be quickly and easily preprocessed for modeling.\n",
    "4. Pipelines can prevent data leakage. Pipelines are designed to only be fit on training data. Later you will learn a technique called 'cross-validation' and pipelines will simplify performing this without leaking data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import set_config\n",
    "\n",
    "set_config(display=\"diagram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2928 entries, Afghanistan2015 to Zimbabwe2000\n",
      "Data columns (total 20 columns):\n",
      " #   Column                           Non-Null Count  Dtype  \n",
      "---  ------                           --------------  -----  \n",
      " 0   Status                           2928 non-null   int64  \n",
      " 1   Life expectancy                  2928 non-null   float64\n",
      " 2   Adult Mortality                  2928 non-null   int64  \n",
      " 3   infant deaths                    2928 non-null   int64  \n",
      " 4   Alcohol                          2735 non-null   float64\n",
      " 5   percentage expenditure           2928 non-null   float64\n",
      " 6   Hepatitis B                      2375 non-null   float64\n",
      " 7   Measles                          2928 non-null   int64  \n",
      " 8   BMI                              2896 non-null   float64\n",
      " 9   under-five deaths                2928 non-null   int64  \n",
      " 10  Polio                            2909 non-null   float64\n",
      " 11  Total expenditure                2702 non-null   float64\n",
      " 12  Diphtheria                       2909 non-null   float64\n",
      " 13  HIV/AIDS                         2928 non-null   float64\n",
      " 14  GDP                              2485 non-null   float64\n",
      " 15  Population                       2284 non-null   float64\n",
      " 16  thinness  1-19 years             2896 non-null   float64\n",
      " 17  thinness 5-9 years               2896 non-null   float64\n",
      " 18  Income composition of resources  2768 non-null   float64\n",
      " 19  Schooling                        2768 non-null   float64\n",
      "dtypes: float64(15), int64(5)\n",
      "memory usage: 480.4+ KB\n",
      "None \n",
      "\n",
      "Status                               0\n",
      "Life expectancy                      0\n",
      "Adult Mortality                      0\n",
      "infant deaths                        0\n",
      "Alcohol                            193\n",
      "percentage expenditure               0\n",
      "Hepatitis B                        553\n",
      "Measles                              0\n",
      "BMI                                 32\n",
      "under-five deaths                    0\n",
      "Polio                               19\n",
      "Total expenditure                  226\n",
      "Diphtheria                          19\n",
      "HIV/AIDS                             0\n",
      "GDP                                443\n",
      "Population                         644\n",
      "thinness  1-19 years                32\n",
      "thinness 5-9 years                  32\n",
      "Income composition of resources    160\n",
      "Schooling                          160\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "path = \"C:/Users/User/Desktop/Life Expectancy Data (all_numeric).csv\"\n",
    "df = pd.read_csv(path, index_col=\"CountryYear\")\n",
    "df.head()\n",
    "\n",
    "# Inspect the data\n",
    "print(df.info(), \"\\n\")\n",
    "print(df.isna().sum())\n",
    "\n",
    "\n",
    "# We can see that several columns are missing data. We will want to impute the missing data before we scale the data, so our pipeline will be ordered as:\n",
    "# Step 1. Imputer\n",
    "# Step 2. Scaler.\n",
    "# All of our data is numeric, so we don't need to one-hot encode the data. We can also use median imputation or mean imputation on all of the columns.\n",
    "# If we wanted to, we COULD use ColumnTransformer to split the columns by integers and floats and apply mean imputation to the floats and median imputation to the integers, and then scale them all. You'll\n",
    "# learn how to combine ColumnTransformer and Pipelines in a future lesson. For this lesson, we will just use a median imputer for all of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 missing values \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -0.81229166, -0.26366021, ..., -0.87868801,\n",
       "         1.19451878,  1.92222335],\n",
       "       [ 0.        ,  1.43809769,  0.15576412, ...,  0.58477555,\n",
       "         0.22791761,  0.08271906],\n",
       "       [ 0.        ,  2.02690924, -0.18501814, ...,  0.87303352,\n",
       "        -0.68443553, -0.80637468],\n",
       "       ...,\n",
       "       [ 0.        , -1.10266448, -0.11511409, ..., -0.10260885,\n",
       "        -0.88170108, -1.17427554],\n",
       "       [ 0.        , -0.73163255, -0.24618419, ..., -0.96738278,\n",
       "         0.97259504,  0.87983758],\n",
       "       [ 0.        ,  1.43003177, -0.20249416, ...,  1.07259673,\n",
       "        -3.11080174, -2.24731971]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will be predicting 'Life expectancy' so we will set that as our y target.\n",
    "\n",
    "# divide features and target and perform a train/test split.\n",
    "X = df.drop(columns=[\"Life expectancy\"])\n",
    "y = df[\"Life expectancy\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# instantiate an imputer and a scaler\n",
    "median_imputer = SimpleImputer(strategy=\"median\")\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# combine the imputer and the scaler into a pipeline\n",
    "preprocessing_pipeline = make_pipeline(median_imputer, scaler)\n",
    "preprocessing_pipeline\n",
    "\n",
    "# fit pipeline on training data\n",
    "preprocessing_pipeline.fit(X_train)\n",
    "\n",
    "# transform train and test sets\n",
    "X_train_processed = preprocessing_pipeline.transform(X_train)\n",
    "X_test_processed = preprocessing_pipeline.transform(X_test)\n",
    "\n",
    "# inspect the result of the transformation\n",
    "print(np.isnan(X_train_processed).sum().sum(), \"missing values \\n\")\n",
    "X_train_processed\n",
    "\n",
    "# Scikit-Learn transformers and pipelines always return Numpy arrays, not Pandas dataframes.\n",
    "# We can use np.isnan(array).sum().sum() (not the method .isna()!) to count the missing values in the resulting array.\n",
    "# We can see that there are no remaining missing values and all of the values seem to be scaled."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
