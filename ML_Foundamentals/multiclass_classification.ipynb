{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class label</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alcalinity of ash</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280/OD315 of diluted wines</th>\n",
       "      <th>Proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class label  Alcohol  Malic acid   Ash  Alcalinity of ash  Magnesium  \\\n",
       "0            1    14.23        1.71  2.43               15.6        127   \n",
       "1            1    13.20        1.78  2.14               11.2        100   \n",
       "2            1    13.16        2.36  2.67               18.6        101   \n",
       "3            1    14.37        1.95  2.50               16.8        113   \n",
       "4            1    13.24        2.59  2.87               21.0        118   \n",
       "\n",
       "   Total phenols  Flavanoids  Nonflavanoid phenols  Proanthocyanins  \\\n",
       "0           2.80        3.06                  0.28             2.29   \n",
       "1           2.65        2.76                  0.26             1.28   \n",
       "2           2.80        3.24                  0.30             2.81   \n",
       "3           3.85        3.49                  0.24             2.18   \n",
       "4           2.80        2.69                  0.39             1.82   \n",
       "\n",
       "   Color intensity   Hue  OD280/OD315 of diluted wines  Proline  \n",
       "0             5.64  1.04                          3.92     1065  \n",
       "1             4.38  1.05                          3.40     1050  \n",
       "2             5.68  1.03                          3.17     1185  \n",
       "3             7.80  0.86                          3.45     1480  \n",
       "4             4.32  1.04                          2.93      735  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_names = ['Class label', 'Alcohol', 'Malic acid', 'Ash', 'Alcalinity of ash', 'Magnesium', 'Total phenols',\n",
    "'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins','Color intensity', 'Hue', 'OD280/OD315 of diluted wines','Proline']\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(r'C:/Users/User/Desktop/wine_info.csv', header = None, names = col_names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiclass classification strategies\n",
    "# While logistic regression is certainly not the only algorithms that were originally meant for binary classification (perceptrons and support vector machines), it is a known weakness of the algorithm. \n",
    "# Whilethere are ways to extend logistic regression to multiclass (more than two class) problems, this section covers general ways to extend binary classification algorithms to multiclass.\n",
    "# The two approaches covered here are the following:\n",
    "# • The One-vs-Rest (One-vs-All) strategy splits a multi-class classification into one binary classification problem per class.\n",
    "# • The One-vs-One strategy splits a multi-class classification into one binary classification problem per each pair of classes.\n",
    "# One versus all theoretical example\n",
    "# For example, given a multi-class classification problem with examples for each class ‘setosa,’ ‘versicolor,’ and ‘virginica‘. This could be divided into three binary classification datasets as follows:\n",
    "# • Binary Classification Problem 1: setosa vs [versicolor, virginica]\n",
    "# • Binary Classification Problem 2: versicolor vs [setosa, virginica]\n",
    "# • Binary Classification Problem 3: virginica vs [setosa, versicolor]\n",
    "# This approach requires that each model predicts a class membership probability or a probability-like score. The argmax of these scores (class index with the largest score) is then used to predict a class.\n",
    "# This approach is commonly used for algorithms that naturally predict numerical class membership probability or score, such as logistic regression.\n",
    "# As such, the implementation of these algorithms in the scikit-learn library implements the OvR strategy by default when using these algorithms for multi-class classification. The scikit-learn library also\n",
    "# provides a separate OneVsRestClassifier class that allows the one-vs-rest strategy to be used with any classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class labels [1 2 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2    71\n",
       "1    59\n",
       "3    48\n",
       "Name: Class label, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print out how many classes\n",
    "print('Class labels', np.unique(df['Class label']))\n",
    "\n",
    "# Classes aren't balanced.\n",
    "df['Class label'].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 41, 2: 50, 3: 33}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Arrange data into features matrix and target vector\n",
    "X = df.loc[:, df.columns[(df.columns != 'Class label')]]\n",
    "\n",
    "y = df.loc[:, 'Class label'].values\n",
    "# In statistical surveys,\n",
    "# when subpopulations within an overall population vary,\n",
    "# it could be advantageous to sample each subpopulation (stratum) independently.\n",
    "# Stratification is the process of dividing members of the population into homogeneous subgroups before sampling.\n",
    "#help(train_test_split)\n",
    "# Split into training and test sets\n",
    "# Providing the class label array y as an argument to stratify ensures both\n",
    "# the training set and test datasets have the same class proportions as the\n",
    "# original dataset\n",
    "X_train, X_test, y_train, y_test =train_test_split(X, y, test_size=0.3, random_state=0, stratify=y)\n",
    "\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 1.0\n",
      "Test accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Standardize Data\n",
    "scaler = StandardScaler()\n",
    "# Fit on training set only.\n",
    "scaler.fit(X_train)\n",
    "# Apply transform to both the training set and the test set.\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "log_reg = LogisticRegression(penalty='l1',\n",
    "C=1.0,\n",
    "solver='liblinear',\n",
    "multi_class='ovr')\n",
    "\n",
    "log_reg.fit(X_train, y_train)\n",
    "print('Training accuracy:', log_reg.score(X_train, y_train))\n",
    "print('Test accuracy:', log_reg.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.24596879,  0.18051402,  0.74618495, -1.16384232,  0.        ,\n",
       "         0.        ,  1.1610691 ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.55688114,  2.50902341],\n",
       "       [-1.53736051, -0.38713904, -0.99556154,  0.36528274, -0.05971364,\n",
       "         0.        ,  0.66852252,  0.        ,  0.        , -1.93442361,\n",
       "         1.23339835,  0.        , -2.23107213],\n",
       "       [ 0.13559719,  0.16841227,  0.35726656,  0.        ,  0.        ,\n",
       "         0.        , -2.43776807,  0.        ,  0.        ,  1.56357362,\n",
       "        -0.81895198, -0.49261971,  0.        ]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Both the training and test accuracies (both 100 percent) indicate that our model does a perfect job on both datasets. When you access the intercept terms via the `log_reg.intercept_` attribute, we see that\n",
    "# the array returns three values.\n",
    "log_reg.intercept_\n",
    "\n",
    "# Since we fit the Logistic Regression object on a multiclass dataset via the OvR approach, the first intercept belongs to the model that fits class 1 versus classes 2 and 3, the second value is the intercept of\n",
    "# the model that fits class 2 versus classes 1 and 3, and the third value is the intercept of the model that fits class 3 versus 1 and 2.\n",
    "log_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How predictions work\n",
    "# Scikit-learn will output probabilities for each class. The highest probability will be the class prediction. So if you have three classes, scikit-learn will output three probabilities for the predict_prob method. If\n",
    "# you have four classes scikit-learn will output 4 probabilities. In this case, we have three classes so the result will give us three probabilities. We choose the class with the highest probability.\n",
    "\n",
    "# The first class is the highest score so it will be the predict 0 (the first class) for this data\n",
    "log_reg.predict_proba(X_test[0:1])\n",
    "\n",
    "# We can get the class associated with the highest probability too\n",
    "log_reg.predict(X_test[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One versus One theoretical example\n",
    "# This approach was used for support vector machines (SVM) and related kernel-based algorithms which isn't covered in this course. This is because the performance of kernel methods does not scale in\n",
    "# proportion to the size of the training dataset and using subsets of the training data may counter this effect.\n",
    "# One-vs-One (OvO for short) is another heuristic method for using binary classification algorithms for multi-class classification.\n",
    "# Like one-vs-rest, one-vs-one splits a multi-class classification dataset into binary classification problems. Unlike one-vs-rest that splits it into one binary dataset for each class, the one-vs-one approach splits\n",
    "# the dataset into one dataset for each class versus every other class.\n",
    "\n",
    "# For example, given a multi-class classification problem with examples for each class ‘setosa,’ ‘versicolor,’ and ‘virginica‘. This could be divided into three binary classification datasets as follows:\n",
    "# • Binary Classification Problem 1: setosa vs versicolor\n",
    "# • Binary Classification Problem 2: versicolor vs virginica\n",
    "# • Binary Classification Problem 3: setosa vs virginica\n",
    "\n",
    "# All of this may not seem very different than One-vs-Rest (One-vs-All), but this method has O(n_classes^2) complexity which means that it is slower to perform. When you have n_classes, you will need to fit\n",
    "# n_classes * (n_classes - 1) / 2 classifiers.\n",
    "\n",
    "# Here is an example to show how out of hand this process can grow. Imagine you have 10 classes which are the digits 0-9. This means we will have to train 45 separate classifiers. Training one model can\n",
    "# take time (depending on the model and how much data you have), but training 45 separate ones is time consuming.\n",
    "# (NumClasses * (NumClasses – 1)) / 2\n",
    "# (10 * (10 – 1)) / 2\n",
    "# (10 * 9) / 2\n",
    "# 90 / 2\n",
    "# 45"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
