{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical Clustering\n",
    "Hierarchical Clustering is another way to cluster our data into different groups.\n",
    "There are two types of Hierarchical Clustering algorithms:\n",
    "1. Divisive Clustering: We can call this the \"top-down\" approach. Here, we start with all of our data in one cluster. From there, we split the data into more and more clusters based on\n",
    "similar traits.\n",
    "2. Agglomerative Clustering: You can think of this as the \"bottom-up\" approach. We start with each data point in its own cluster. From there, we group clusters together until we eventually\n",
    "have only one cluster.\n",
    "In both of these algorithms, we have to choose how many clusters we want to ultimately end up with. We can visualize what we might think is an optimal number of clusters using a tree-like\n",
    "diagram called a dendrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import scipy.cluster.hierarchy as sch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = pd.read_csv('/content/drive/path_to_dataset/modified_wine.csv')\n",
    "df = wine[['malic_acid', 'flavanoids']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Standard Scaler\n",
    "scaler = StandardScaler()\n",
    "# Fit & transform data.\n",
    "scaled_df = scaler.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Dendogram\n",
    "plt.figure(figsize = (15, 5))\n",
    "sch.dendrogram(sch.linkage(scaled_df, method = 'ward'))\n",
    "plt.xlabel('Data Points');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing Clusters\n",
    "One way to choose the number of clusters that an agglomerative clustering algorithm stops at is to examine this dendrogram.\n",
    "We can use the below dendrogram to determine the optimal number of clusters. To do this we examine the dendrogram and find the place where the vertical lines are, on average, the\n",
    "longest and draw a horizontal line at that place. The number of vertical lines that it intersects is might be a good number of clusters. In the above dendrogram, it appears that the longest\n",
    "lines, on average, are between 6 and 11 on the y axis. We could draw our line at 8 and have the algorithm stop at 3 clusters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
