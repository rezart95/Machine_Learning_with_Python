{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early Stopping\n",
    "Early stopping is exactly what it sounds like - we stop training the model early. Instead of training for a certain number of epochs, we train the model until the validation loss begins to increase. This ensures\n",
    "our model does not keep overfitting as we go through more and more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data\n",
    "df = pd.read_csv('/content/drive/path_to_data/nba.csv', index_col = 'Name')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop missings\n",
    "df.dropna(inplace = True)\n",
    "# Save X data\n",
    "X = df.drop(columns = 'TARGET_5Yrs')\n",
    "# Encode our target\n",
    "y = df['TARGET_5Yrs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale our data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feedforward neural network\n",
    "# Step 1: Define our network structure\n",
    "# Save the number of features we have as our input shape\n",
    "input_shape = X_train.shape[1]\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without early stopping\n",
    "# Sequential model\n",
    "model = Sequential()\n",
    "# First hidden layer\n",
    "model.add(Dense(19, # How many neurons you have in your first hidden layer\n",
    "\n",
    "input_dim = input_shape, # What is the shape of your input features (number of columns)\n",
    "activation = 'relu')) # What activation function are you using?\n",
    "\n",
    "model.add(Dense(10,\n",
    "\n",
    "activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.compile(loss = 'bce', optimizer = 'adam')\n",
    "history = model.fit(X_train, y_train,\n",
    "\n",
    "validation_data = (X_test, y_test),\n",
    "epochs=100)\n",
    "\n",
    "# Visualize the loss\n",
    "plt.plot(history.history['loss'], label='Train loss')\n",
    "plt.plot(history.history['val_loss'], label='Test Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yikes, our model is super overfit! Notice how the training loss continues to decrease while the testing loss begins to increase as we increase the number of epochs we train our model for. This is a super\n",
    "# common problem with neural networks and tells us that our model is overfit and is not performing well on unseen data.\n",
    "# Let's build this same model with early stopping to try to prevent overfitting by stopping when the validation loss begins to increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With early stopping\n",
    "# Sequential model\n",
    "model = Sequential()\n",
    "# First hidden layer\n",
    "\n",
    "model.add(Dense(19, # How many neurons you have in your first hidden layer\n",
    "\n",
    "input_dim = input_shape, # What is the shape of your input features (number of columns)\n",
    "activation = 'relu')) # What activation function are you using?\n",
    "\n",
    "model.add(Dense(10,\n",
    "\n",
    "activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.compile(loss = 'bce', optimizer = 'adam')\n",
    "early_stopping = EarlyStopping(patience = 5)\n",
    "history = model.fit(X_train, y_train,\n",
    "\n",
    "validation_data = (X_test, y_test),\n",
    "epochs=100,\n",
    "callbacks = [early_stopping])\n",
    "\n",
    "# Visualize the loss\n",
    "plt.plot(history.history['loss'], label='Train loss')\n",
    "plt.plot(history.history['val_loss'], label='Test Loss')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is way less overfit because we only trained for about 18 epochs (yours might have trained for a slightly different number of epochs). Early stopping can prevent overfitting and save you training time.\n",
    "# You can also use dropout and early stopping in the same model."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
