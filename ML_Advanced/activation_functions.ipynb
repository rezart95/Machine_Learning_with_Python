{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why Use Activation Functions?\n",
    "During forward propagation, after the inputs to each node are multiplied by the nodes weights for those inputs and they are summed together with the bias, the\n",
    "node applies an activation function before sending its output to the next layer.\n",
    "Activation functions are important because they can create non-linear functions to predict classes or values. Without an activation function a node is essentially\n",
    "a linear regression algorithm. A linear regression can only apply a linear function to a set of inputs. If the problem cannot be modeled with a straight line, a\n",
    "linear regression cannot effectively model it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid:\n",
    "The sigmoid function maps all output values of the node to a value between 0 and 1, as shown above on the y axis of the first plot. This is equivalent to the\n",
    "probabilities predicted by a logistic regression model with LogisticRegression().predict_proba_(X) in sklearn.\n",
    "Tanh (or hyperbolic tangent)\n",
    "Tanh maps the output of a node to a value between -1 and 1, as shown above on the y axis of the second plot.\n",
    "\n",
    "ReLU (Rectified Linear Unit)\n",
    "The ReLU activation function is very popular with machine learning engineers because it performs well. It maps all negative outputs from a node to 0 and\n",
    "returns all positive outputs as is (linear function), as shown above on the third plot. It’s thought that the excellent performance is due to some nodes being\n",
    "‘switched off’ by mapping their values to 0. This is a form of regularization akin to dropout, which you will learn about in another lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation Functions in Model Development\n",
    "Activation functions are hyperparameters set by the engineer. Part of model development is tuning the activation functions for the input and hidden layers to find\n",
    "the ones that perform the best. ReLU is a good place to start.\n",
    "Output Layer Activation Functions\n",
    "The exception is the output layer. Specific activation functions are required for a model to produce specific kinds of outputs.\n",
    "Linear (regression models): If the model should have a continuous output that can be positive or negative. A linear activation function simply returns the value\n",
    "that was passed to it. g(z) = z.\n",
    "Sigmoid (binary classification models): if the model should only output a 0 or a 1, such as in binary classification problems.\n",
    "Softmax (multiclass classification models): if the model should return the most likely candidate from a finite list of options, such as a multiclass classification\n",
    "problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary\n",
    "Activation functions are applied after the weights and inputs are multiplied and summed together with the bias term for each node to determine the node’s\n",
    "output. They allow a model to find a nonlinear function mapping the input features to the target label. While the activation functions of input and hidden layers\n",
    "are tunable, the activation function for the output layer depends on the kind of problem being modeled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
