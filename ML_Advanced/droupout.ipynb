{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning Regularization\n",
    "Neural networks are very prone to overfitting. In order to combat this, we need to regularize so that our model is not too overfit to the training data and is able to perform well on new data as well.\n",
    "Three common regularization techniques for deep learning include:\n",
    " Dropout\n",
    " Early stopping\n",
    " L1/L2 regularization\n",
    "Dropout\n",
    "One of the most common forms of regularization is dropout. What this does is drops out a portion of the neurons so that the model does not learn weights and biases that are too perfect for the training set.\n",
    "Visually, a dropout layer looks like this:\n",
    "\n",
    "Notice that in the dropout layer, each neuron has a 50% probability (p = 0.5) of not being included/updated in that epoch. When we finalize our model and run it through the testing data, we include all of the\n",
    "neurons and do not drop any out.\n",
    "Dropout in Keras\n",
    "Let's try this in Keras! We will look at a neural network with and without dropout.\n",
    "Note: you can watch a video walkthrough of this code at the end of this module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "\n",
    "# We will use the NBA rookie data again to predict whether or not we think a rookie will last at least 5 years in the league. \n",
    "\n",
    "df = pd.read_csv('/content/drive/path_to_data/nba.csv', index_col = 'Name')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data & split into X & y\n",
    "# Drop missings\n",
    "df.dropna(inplace = True)\n",
    "# Save X data\n",
    "X = df.drop(columns = 'TARGET_5Yrs')\n",
    "# Encode our target\n",
    "y = df['TARGET_5Yrs']\n",
    "\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale our data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define our network structure\n",
    "# Save the number of features we have as our input shape\n",
    "input_shape = X_train.shape[1]\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without dropout\n",
    "# Sequential model\n",
    "model = Sequential()\n",
    "# First hidden layer\n",
    "model.add(Dense(19, # How many neurons you have in your first hidden layer\n",
    "                input_dim = input_shape, # What is the shape of your input features (number of columns)\n",
    "                activation = 'relu')) # What activation function are you using?\n",
    "model.add(Dense(10, \n",
    "                activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.compile(loss = 'bce', optimizer = 'adam')\n",
    "history = model.fit(X_train, y_train,\n",
    "                    validation_data = (X_test, y_test), \n",
    "                    epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the loss\n",
    "plt.plot(history.history['loss'], label='Train loss')\n",
    "plt.plot(history.history['val_loss'], label='Test Loss')\n",
    "plt.legend();\n",
    "\n",
    "\n",
    "# Yikes, our model is super overfit! Notice how the training loss continues to decrease while the testing loss begins to increase as we increase the number of epochs we train our model for. This is a super common problem with neural networks and tells us that our model is overfit and is not performing well on unseen data.\n",
    "# Let's build this same model with dropout to try to prevent overfitting. Dropout in Keras is coded as another layer, after the layer you would like to dropout. You need to specify the probability of dropout as well (the probability that each individual neuron has to dropout of the training that epoch).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With dropout\n",
    "# Sequential model\n",
    "model = Sequential()\n",
    "# First hidden layer\n",
    "model.add(Dense(19, # How many neurons you have in your first hidden layer\n",
    "                input_dim = input_shape, # What is the shape of your input features (number of columns)\n",
    "                activation = 'relu')) # What activation function are you using?\n",
    "model.add(Dropout(.2))\n",
    "model.add(Dense(10, \n",
    "                activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.compile(loss = 'bce', optimizer = 'adam')\n",
    "history = model.fit(X_train, y_train,\n",
    "                    validation_data = (X_test, y_test), \n",
    "                    epochs=100)\n",
    "# Visualize the loss\n",
    "plt.plot(history.history['loss'], label='Train loss')\n",
    "plt.plot(history.history['val_loss'], label='Test Loss')\n",
    "plt.legend();"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
