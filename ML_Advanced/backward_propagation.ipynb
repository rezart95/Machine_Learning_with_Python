{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocab:\n",
    " Node: one unit that processes incoming information and produces a single output number\n",
    " Layer: a collection of nodes working in parallel\n",
    " Input Layer: the first set of nodes that work on the features of the input sample\n",
    " Output Layer: the last set of nodes that output a prediction\n",
    " Forward Propagation: the processing and passing of information forward through all layers to produce an output prediction\n",
    " Yhat: the predictions of a model for all samples in X.\n",
    " Cost: the combination of all of a model’s errors on all samples.\n",
    " Cost Function or Loss Function: the difference between the predictions of all samples and their true labels\n",
    " Backward Propagation: the process of updating the weights of each node to reduce the cost function\n",
    " Epoch: One full cycle of forward propagation and backward propagation for all training samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model as a Function:\n",
    "A predictive model is a function. A function is a broad term that describes anything that takes an input and produces a unique output.\n",
    "Many inputs can produce the same output, but each input can only produce one output.\n",
    "A predictive model maps an input, X, with a prediction, yhat.\n",
    "model(X) = yhat.\n",
    "But since we know the true values of the labels, y, we can add the cost function to the predictive model function to make a new function\n",
    "that maps all of the sample X inputs to the cost function.\n",
    "f(X) = y - g(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Little Calculus:\n",
    "There is a very cool tool in calculus called differential equations. A differential equation can tell us how the output of a function changes\n",
    "for a single given input by finding the derivative of that function. With this tool we can determine how we need to change the model in\n",
    "order to change the cost function. In other words, it tells us how the weights in the output layer need to change in order to reduce the\n",
    "total error the model is making over all samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent\n",
    "Changing the weights to reduce the cost function is called Gradient Descent. It’s called that because we can visualize the cost function\n",
    "like a topographical map where lower error rates are ‘down’. Changing the weights to reduce the cost function is like a ball rolling\n",
    "downhill.\n",
    "\n",
    "Image Source\n",
    "The above image shows how, with each epoch the model reduces the cost function, or the errors it makes in predictions. Now, it\n",
    "doesn’t know what the cost function ACTUALLY is, the differential equations only tell it, for a given value of the weights, which way is\n",
    "\n",
    "down and how to change the weight to reduce the cost. It doesn’t know which value for the weight will actually produce the least error.\n",
    "So, it tries over and over, each time changing the weight a little to reduce the cost, like a ball rolling downhill.\n",
    "The above image shows gradient descent for a single weight, but our models have many weights for many inputs. The cost function\n",
    "gains a dimension for each weight in a node.\n",
    "If we had 2 different weights to change, cost function and gradient descent would look more like the image below. Calculus also has\n",
    "equations for how to find how each weight needs to change when there are many inputs to a function. These are called partial\n",
    "derivatives because they are just a part of the derivative of the entire function. In backpropagation the model uses partial differential\n",
    "equations to figure out how to change each weight individually to achieve gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backward Propagation:\n",
    "Okay, so we took a look at how gradient descent works to adjust the weights in the output layer, but what about the layers before it?\n",
    "The algorithm for backpropagation allows us to determine how each layer before the output layer needs to change based on how the\n",
    "one in front of it changed to reduce the cost function. The changes in the weights propagate backward through the layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary:\n",
    "Once the forward propagation step of a model has produced predictions for all samples, the predictions are compared with the true\n",
    "values for each sample to produce a cost. The cost is the combination of all of the errors the model made on all of the samples. The\n",
    "function mapping the inputs to the model to the cost is called the cost function. The model then uses gradient descent to reduce the\n",
    "cost by adjusting the weights in each layer starting with the output layer and moving backward toward the input layer. One full cycle of\n",
    "forward propagation and backward propagation is called one epoch. A model goes through many epochs, changing its internal weights\n",
    "each time to reduce the cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
